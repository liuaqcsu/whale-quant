{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MySQLdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1c5f2ec60401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMySQLdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#import sshtunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#from sshtunnel import SSHTunnelForwarder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MySQLdb'"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob, os\n",
    "import json\n",
    "\n",
    "import MySQLdb \n",
    "#import sshtunnel\n",
    "#from sshtunnel import SSHTunnelForwarder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#from datetime import date \n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "\n",
    "import pandas as pd\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import plotting\n",
    "\n",
    "import MySQLdb \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: quandl in /Applications/anaconda3/lib/python3.7/site-packages (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.14 in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.8 in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (2.8.1)\n",
      "Requirement already satisfied: requests>=2.7.0 in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (2.22.0)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (0.5.1)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (1.14.0)\n",
      "Requirement already satisfied: more-itertools in /Applications/anaconda3/lib/python3.7/site-packages (from quandl) (8.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas>=0.14->quandl) (2019.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->quandl) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Applications/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->quandl) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->quandl) (2019.11.28)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fbprophet in /Applications/anaconda3/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: Cython>=0.22 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (0.29.15)\n",
      "Requirement already satisfied: cmdstanpy==0.9.5 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (0.9.5)\n",
      "Requirement already satisfied: pystan>=2.14 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (2.19.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (1.18.1)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (1.1.3)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (3.1.3)\n",
      "Requirement already satisfied: LunarCalendar>=0.0.9 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (0.0.9)\n",
      "Requirement already satisfied: convertdate>=2.1.2 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (2.2.2)\n",
      "Requirement already satisfied: holidays>=0.10.2 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (0.10.3)\n",
      "Requirement already satisfied: setuptools-git>=1.2 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (2.8.1)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /Applications/anaconda3/lib/python3.7/site-packages (from fbprophet) (4.42.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas>=1.0.4->fbprophet) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Applications/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (2.4.6)\n",
      "Requirement already satisfied: ephem>=3.7.5.3 in /Applications/anaconda3/lib/python3.7/site-packages (from LunarCalendar>=0.0.9->fbprophet) (3.7.7.1)\n",
      "Requirement already satisfied: pymeeus<=1,>=0.3.6 in /Applications/anaconda3/lib/python3.7/site-packages (from convertdate>=2.1.2->fbprophet) (0.3.7)\n",
      "Requirement already satisfied: korean-lunar-calendar in /Applications/anaconda3/lib/python3.7/site-packages (from holidays>=0.10.2->fbprophet) (0.2.1)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.7/site-packages (from holidays>=0.10.2->fbprophet) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (46.0.0.post20200309)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrends\n",
      "  Downloading pytrends-4.7.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/lib/python3.7/site-packages (from pytrends) (2.22.0)\n",
      "Requirement already satisfied: lxml in /Applications/anaconda3/lib/python3.7/site-packages (from pytrends) (4.5.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /Applications/anaconda3/lib/python3.7/site-packages (from pytrends) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->pytrends) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->pytrends) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->pytrends) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->pytrends) (2.8)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas>=0.25->pytrends) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas>=0.25->pytrends) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.14.0)\n",
      "Installing collected packages: pytrends\n",
      "Successfully installed pytrends-4.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quandl for financial analysis, pandas and numpy for data manipulation\n",
    "# fbprophet for additive models, #pytrends for Google trend data\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fbprophet\n",
    "import pytrends\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# matplotlib pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# Class for analyzing and (attempting) to predict future prices\n",
    "# Contains a number of visualizations and analysis methods\n",
    "class Stocker():\n",
    "    \n",
    "    # Initialization requires a ticker symbol\n",
    "    def __init__(self, ticker, exchange='WIKI'):\n",
    "        \n",
    "        # Enforce capitalization\n",
    "        ticker = ticker.upper()\n",
    "        \n",
    "        # Symbol is used for labeling plots\n",
    "        self.symbol = ticker\n",
    "        \n",
    "        # Use Personal Api Key\n",
    "        # quandl.ApiConfig.api_key = 'YourKeyHere'\n",
    "\n",
    "        # Retrieval the financial data\n",
    "        try:\n",
    "            stock = quandl.get('%s/%s' % (exchange, ticker))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('Error Retrieving Data.')\n",
    "            print(e)\n",
    "            return\n",
    "        \n",
    "        # Set the index to a column called Date\n",
    "        stock = stock.reset_index(level=0)\n",
    "        \n",
    "        # Columns required for prophet\n",
    "        stock['ds'] = stock['Date']\n",
    "\n",
    "        if ('Adj. Close' not in stock.columns):\n",
    "            stock['Adj. Close'] = stock['Close']\n",
    "            stock['Adj. Open'] = stock['Open']\n",
    "        \n",
    "        stock['y'] = stock['Adj. Close']\n",
    "        stock['Daily Change'] = stock['Adj. Close'] - stock['Adj. Open']\n",
    "        \n",
    "        # Data assigned as class attribute\n",
    "        self.stock = stock.copy()\n",
    "        \n",
    "        # Minimum and maximum date in range\n",
    "        self.min_date = min(stock['Date'])\n",
    "        self.max_date = max(stock['Date'])\n",
    "        \n",
    "        # Find max and min prices and dates on which they occurred\n",
    "        self.max_price = np.max(self.stock['y'])\n",
    "        self.min_price = np.min(self.stock['y'])\n",
    "        \n",
    "        self.min_price_date = self.stock[self.stock['y'] == self.min_price]['Date']\n",
    "        self.min_price_date = self.min_price_date[self.min_price_date.index[0]]\n",
    "        self.max_price_date = self.stock[self.stock['y'] == self.max_price]['Date']\n",
    "        self.max_price_date = self.max_price_date[self.max_price_date.index[0]]\n",
    "        \n",
    "        # The starting price (starting with the opening price)\n",
    "        self.starting_price = float(self.stock.ix[0, 'Adj. Open'])\n",
    "        \n",
    "        # The most recent price\n",
    "        self.most_recent_price = float(self.stock.ix[len(self.stock) - 1, 'y'])\n",
    "\n",
    "        # Whether or not to round dates\n",
    "        self.round_dates = True\n",
    "        \n",
    "        # Number of years of data to train on\n",
    "        self.training_years = 3\n",
    "\n",
    "        # Prophet parameters\n",
    "        # Default prior from library\n",
    "        self.changepoint_prior_scale = 0.05 \n",
    "        self.weekly_seasonality = False\n",
    "        self.daily_seasonality = False\n",
    "        self.monthly_seasonality = True\n",
    "        self.yearly_seasonality = True\n",
    "        self.changepoints = None\n",
    "        \n",
    "        print('{} Stocker Initialized. Data covers {} to {}.'.format(self.symbol,\n",
    "                                                                     self.min_date.date(),\n",
    "                                                                     self.max_date.date()))\n",
    "    \n",
    "    \"\"\"\n",
    "    Make sure start and end dates are in the range and can be\n",
    "    converted to pandas datetimes. Returns dates in the correct format\n",
    "    \"\"\"\n",
    "    def handle_dates(self, start_date, end_date):\n",
    "        \n",
    "        \n",
    "        # Default start and end date are the beginning and end of data\n",
    "        if start_date is None:\n",
    "            start_date = self.min_date\n",
    "        if end_date is None:\n",
    "            end_date = self.max_date\n",
    "        \n",
    "        try:\n",
    "            # Convert to pandas datetime for indexing dataframe\n",
    "            start_date = pd.to_datetime(start_date)\n",
    "            end_date = pd.to_datetime(end_date)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('Enter valid pandas date format.')\n",
    "            print(e)\n",
    "            return\n",
    "        \n",
    "        valid_start = False\n",
    "        valid_end = False\n",
    "        \n",
    "        # User will continue to enter dates until valid dates are met\n",
    "        while (not valid_start) & (not valid_end):\n",
    "            valid_end = True\n",
    "            valid_start = True\n",
    "            \n",
    "            if end_date.date() < start_date.date():\n",
    "                print('End Date must be later than start date.')\n",
    "                start_date = pd.to_datetime(input('Enter a new start date: '))\n",
    "                end_date= pd.to_datetime(input('Enter a new end date: '))\n",
    "                valid_end = False\n",
    "                valid_start = False\n",
    "            \n",
    "            else: \n",
    "                if end_date.date() > self.max_date.date():\n",
    "                    print('End Date exceeds data range')\n",
    "                    end_date= pd.to_datetime(input('Enter a new end date: '))\n",
    "                    valid_end = False\n",
    "\n",
    "                if start_date.date() < self.min_date.date():\n",
    "                    print('Start Date is before date range')\n",
    "                    start_date = pd.to_datetime(input('Enter a new start date: '))\n",
    "                    valid_start = False\n",
    "                \n",
    "        \n",
    "        return start_date, end_date\n",
    "        \n",
    "    \"\"\"\n",
    "    Return the dataframe trimmed to the specified range.\n",
    "    \"\"\"\n",
    "    def make_df(self, start_date, end_date, df=None):\n",
    "        \n",
    "        # Default is to use the object stock data\n",
    "        if not df:\n",
    "            df = self.stock.copy()\n",
    "        \n",
    "        \n",
    "        start_date, end_date = self.handle_dates(start_date, end_date)\n",
    "        \n",
    "        # keep track of whether the start and end dates are in the data\n",
    "        start_in = True\n",
    "        end_in = True\n",
    "\n",
    "        # If user wants to round dates (default behavior)\n",
    "        if self.round_dates:\n",
    "            # Record if start and end date are in df\n",
    "            if (start_date not in list(df['Date'])):\n",
    "                start_in = False\n",
    "            if (end_date not in list(df['Date'])):\n",
    "                end_in = False\n",
    "\n",
    "            # If both are not in dataframe, round both\n",
    "            if (not end_in) & (not start_in):\n",
    "                trim_df = df[(df['Date'] >= start_date.date()) & \n",
    "                             (df['Date'] <= end_date.date())]\n",
    "            \n",
    "            else:\n",
    "                # If both are in dataframe, round neither\n",
    "                if (end_in) & (start_in):\n",
    "                    trim_df = df[(df['Date'] >= start_date.date()) & \n",
    "                                 (df['Date'] <= end_date.date())]\n",
    "                else:\n",
    "                    # If only start is missing, round start\n",
    "                    if (not start_in):\n",
    "                        trim_df = df[(df['Date'] > start_date.date()) & \n",
    "                                     (df['Date'] <= end_date.date())]\n",
    "                    # If only end is imssing round end\n",
    "                    elif (not end_in):\n",
    "                        trim_df = df[(df['Date'] >= start_date.date()) & \n",
    "                                     (df['Date'] < end_date.date())]\n",
    "\n",
    "        \n",
    "        else:\n",
    "            valid_start = False\n",
    "            valid_end = False\n",
    "            while (not valid_start) & (not valid_end):\n",
    "                start_date, end_date = self.handle_dates(start_date, end_date)\n",
    "                \n",
    "                # No round dates, if either data not in, print message and return\n",
    "                if (start_date in list(df['Date'])):\n",
    "                    valid_start = True\n",
    "                if (end_date in list(df['Date'])):\n",
    "                    valid_end = True\n",
    "                    \n",
    "                # Check to make sure dates are in the data\n",
    "                if (start_date not in list(df['Date'])):\n",
    "                    print('Start Date not in data (either out of range or not a trading day.)')\n",
    "                    start_date = pd.to_datetime(input(prompt='Enter a new start date: '))\n",
    "                    \n",
    "                elif (end_date not in list(df['Date'])):\n",
    "                    print('End Date not in data (either out of range or not a trading day.)')\n",
    "                    end_date = pd.to_datetime(input(prompt='Enter a new end date: ') )\n",
    "\n",
    "            # Dates are not rounded\n",
    "            trim_df = df[(df['Date'] >= start_date.date()) & \n",
    "                         (df['Date'] <= end_date.date())]\n",
    "\n",
    "        \n",
    "            \n",
    "        return trim_df\n",
    "\n",
    "\n",
    "    # Basic Historical Plots and Basic Statistics\n",
    "    def plot_stock(self, start_date=None, end_date=None, stats=['Adj. Close'], plot_type='basic'):\n",
    "        \n",
    "        self.reset_plot()\n",
    "        \n",
    "        if start_date is None:\n",
    "            start_date = self.min_date\n",
    "        if end_date is None:\n",
    "            end_date = self.max_date\n",
    "        \n",
    "        stock_plot = self.make_df(start_date, end_date)\n",
    "\n",
    "        colors = ['r', 'b', 'g', 'y', 'c', 'm']\n",
    "        \n",
    "        for i, stat in enumerate(stats):\n",
    "            \n",
    "            stat_min = min(stock_plot[stat])\n",
    "            stat_max = max(stock_plot[stat])\n",
    "\n",
    "            stat_avg = np.mean(stock_plot[stat])\n",
    "            \n",
    "            date_stat_min = stock_plot[stock_plot[stat] == stat_min]['Date']\n",
    "            date_stat_min = date_stat_min[date_stat_min.index[0]].date()\n",
    "            date_stat_max = stock_plot[stock_plot[stat] == stat_max]['Date']\n",
    "            date_stat_max = date_stat_max[date_stat_max.index[0]].date()\n",
    "            \n",
    "            print('Maximum {} = {:.2f} on {}.'.format(stat, stat_max, date_stat_max))\n",
    "            print('Minimum {} = {:.2f} on {}.'.format(stat, stat_min, date_stat_min))\n",
    "            print('Current {} = {:.2f} on {}.\\n'.format(stat, self.stock.ix[len(self.stock) - 1, stat], self.max_date.date()))\n",
    "            \n",
    "            # Percentage y-axis\n",
    "            if plot_type == 'pct':\n",
    "                # Simple Plot \n",
    "                plt.style.use('fivethirtyeight');\n",
    "                if stat == 'Daily Change':\n",
    "                    plt.plot(stock_plot['Date'], 100 * stock_plot[stat],\n",
    "                         color = colors[i], linewidth = 2.4, alpha = 0.9,\n",
    "                         label = stat)\n",
    "                else:\n",
    "                    plt.plot(stock_plot['Date'], 100 * (stock_plot[stat] -  stat_avg) / stat_avg,\n",
    "                         color = colors[i], linewidth = 2.4, alpha = 0.9,\n",
    "                         label = stat)\n",
    "\n",
    "                plt.xlabel('Date'); plt.ylabel('Change Relative to Average (%)'); plt.title('%s Stock History' % self.symbol); \n",
    "                plt.legend(prop={'size':10})\n",
    "                plt.grid(color = 'k', alpha = 0.4); \n",
    "\n",
    "            # Stat y-axis\n",
    "            elif plot_type == 'basic':\n",
    "                plt.style.use('fivethirtyeight');\n",
    "                plt.plot(stock_plot['Date'], stock_plot[stat], color = colors[i], linewidth = 3, label = stat, alpha = 0.8)\n",
    "                plt.xlabel('Date'); plt.ylabel('US $'); plt.title('%s Stock History' % self.symbol); \n",
    "                plt.legend(prop={'size':10})\n",
    "                plt.grid(color = 'k', alpha = 0.4); \n",
    "      \n",
    "        plt.show();\n",
    "        \n",
    "    # Reset the plotting parameters to clear style formatting\n",
    "    # Not sure if this should be a static method\n",
    "    @staticmethod\n",
    "    def reset_plot():\n",
    "        \n",
    "        # Restore default parameters\n",
    "        matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "        \n",
    "        # Adjust a few parameters to liking\n",
    "        matplotlib.rcParams['figure.figsize'] = (8, 5)\n",
    "        matplotlib.rcParams['axes.labelsize'] = 10\n",
    "        matplotlib.rcParams['xtick.labelsize'] = 8\n",
    "        matplotlib.rcParams['ytick.labelsize'] = 8\n",
    "        matplotlib.rcParams['axes.titlesize'] = 14\n",
    "        matplotlib.rcParams['text.color'] = 'k'\n",
    "    \n",
    "    # Method to linearly interpolate prices on the weekends\n",
    "    def resample(self, dataframe):\n",
    "        # Change the index and resample at daily level\n",
    "        dataframe = dataframe.set_index('ds')\n",
    "        dataframe = dataframe.resample('D')\n",
    "        \n",
    "        # Reset the index and interpolate nan values\n",
    "        dataframe = dataframe.reset_index(level=0)\n",
    "        dataframe = dataframe.interpolate()\n",
    "        return dataframe\n",
    "    \n",
    "    # Remove weekends from a dataframe\n",
    "    def remove_weekends(self, dataframe):\n",
    "        \n",
    "        # Reset index to use ix\n",
    "        dataframe = dataframe.reset_index(drop=True)\n",
    "        \n",
    "        weekends = []\n",
    "        \n",
    "        # Find all of the weekends\n",
    "        for i, date in enumerate(dataframe['ds']):\n",
    "            if (date.weekday()) == 5 | (date.weekday() == 6):\n",
    "                weekends.append(i)\n",
    "            \n",
    "        # Drop the weekends\n",
    "        dataframe = dataframe.drop(weekends, axis=0)\n",
    "        \n",
    "        return dataframe\n",
    "    \n",
    "    \n",
    "    # Calculate and plot profit from buying and holding shares for specified date range\n",
    "    def buy_and_hold(self, start_date=None, end_date=None, nshares=1):\n",
    "        self.reset_plot()\n",
    "        \n",
    "        start_date, end_date = self.handle_dates(start_date, end_date)\n",
    "            \n",
    "        # Find starting and ending price of stock\n",
    "        start_price = float(self.stock[self.stock['Date'] == start_date]['Adj. Open'])\n",
    "        end_price = float(self.stock[self.stock['Date'] == end_date]['Adj. Close'])\n",
    "        \n",
    "        # Make a profit dataframe and calculate profit column\n",
    "        profits = self.make_df(start_date, end_date)\n",
    "        profits['hold_profit'] = nshares * (profits['Adj. Close'] - start_price)\n",
    "        \n",
    "        # Total profit\n",
    "        total_hold_profit = nshares * (end_price - start_price)\n",
    "        \n",
    "        print('{} Total buy and hold profit from {} to {} for {} shares = ${:.2f}'.format\n",
    "              (self.symbol, start_date.date(), end_date.date(), nshares, total_hold_profit))\n",
    "        \n",
    "        # Plot the total profits \n",
    "        plt.style.use('dark_background')\n",
    "        \n",
    "        # Location for number of profit\n",
    "        text_location = (end_date - pd.DateOffset(months = 1)).date()\n",
    "        \n",
    "        # Plot the profits over time\n",
    "        plt.plot(profits['Date'], profits['hold_profit'], 'b', linewidth = 3)\n",
    "        plt.ylabel('Profit ($)'); plt.xlabel('Date'); plt.title('Buy and Hold Profits for {} {} to {}'.format(\n",
    "                                                                self.symbol, start_date.date(), end_date.date()))\n",
    "        \n",
    "        # Display final value on graph\n",
    "        plt.text(x = text_location, \n",
    "             y =  total_hold_profit + (total_hold_profit / 40),\n",
    "             s = '$%d' % total_hold_profit,\n",
    "            color = 'g' if total_hold_profit > 0 else 'r',\n",
    "            size = 14)\n",
    "        \n",
    "        plt.grid(alpha=0.2)\n",
    "        plt.show();\n",
    "        \n",
    "    # Create a prophet model without training\n",
    "    def create_model(self):\n",
    "\n",
    "        # Make the model\n",
    "        model = fbprophet.Prophet(daily_seasonality=self.daily_seasonality,  \n",
    "                                  weekly_seasonality=self.weekly_seasonality, \n",
    "                                  yearly_seasonality=self.yearly_seasonality,\n",
    "                                  changepoint_prior_scale=self.changepoint_prior_scale,\n",
    "                                  changepoints=self.changepoints)\n",
    "        \n",
    "        if self.monthly_seasonality:\n",
    "            # Add monthly seasonality\n",
    "            model.add_seasonality(name = 'monthly', period = 30.5, fourier_order = 5)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Graph the effects of altering the changepoint prior scale (cps)\n",
    "    def changepoint_prior_analysis(self, changepoint_priors=[0.001, 0.05, 0.1, 0.2], colors=['b', 'r', 'grey', 'gold']):\n",
    "    \n",
    "        # Training and plotting with specified years of data\n",
    "        train = self.stock[(self.stock['Date'] > (max(self.stock['Date']) - pd.DateOffset(years=self.training_years)).date())]\n",
    "        \n",
    "        # Iterate through all the changepoints and make models\n",
    "        for i, prior in enumerate(changepoint_priors):\n",
    "            # Select the changepoint\n",
    "            self.changepoint_prior_scale = prior\n",
    "            \n",
    "            # Create and train a model with the specified cps\n",
    "            model = self.create_model()\n",
    "            model.fit(train)\n",
    "            future = model.make_future_dataframe(periods=180, freq='D')\n",
    "            \n",
    "            # Make a dataframe to hold predictions\n",
    "            if i == 0:\n",
    "                predictions = future.copy()\n",
    "                \n",
    "            future = model.predict(future)\n",
    "            \n",
    "            # Fill in prediction dataframe\n",
    "            predictions['%.3f_yhat_upper' % prior] = future['yhat_upper']\n",
    "            predictions['%.3f_yhat_lower' % prior] = future['yhat_lower']\n",
    "            predictions['%.3f_yhat' % prior] = future['yhat']\n",
    "         \n",
    "        # Remove the weekends\n",
    "        predictions = self.remove_weekends(predictions)\n",
    "        \n",
    "        # Plot set-up\n",
    "        self.reset_plot()\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        \n",
    "        # Actual observations\n",
    "        ax.plot(train['ds'], train['y'], 'ko', ms = 4, label = 'Observations')\n",
    "        color_dict = {prior: color for prior, color in zip(changepoint_priors, colors)}\n",
    "\n",
    "        # Plot each of the changepoint predictions\n",
    "        for prior in changepoint_priors:\n",
    "            # Plot the predictions themselves\n",
    "            ax.plot(predictions['ds'], predictions['%.3f_yhat' % prior], linewidth = 1.2,\n",
    "                     color = color_dict[prior], label = '%.3f prior scale' % prior)\n",
    "            \n",
    "            # Plot the uncertainty interval\n",
    "            ax.fill_between(predictions['ds'].dt.to_pydatetime(), predictions['%.3f_yhat_upper' % prior],\n",
    "                            predictions['%.3f_yhat_lower' % prior], facecolor = color_dict[prior],\n",
    "                            alpha = 0.3, edgecolor = 'k', linewidth = 0.6)\n",
    "                            \n",
    "        # Plot labels\n",
    "        plt.legend(loc = 2, prop={'size': 10})\n",
    "        plt.xlabel('Date'); plt.ylabel('Stock Price ($)'); plt.title('Effect of Changepoint Prior Scale');\n",
    "        plt.show()\n",
    "            \n",
    "    # Basic prophet model for specified number of days  \n",
    "    def create_prophet_model(self, days=0, resample=False):\n",
    "        \n",
    "        self.reset_plot()\n",
    "        \n",
    "        model = self.create_model()\n",
    "        \n",
    "        # Fit on the stock history for self.training_years number of years\n",
    "        stock_history = self.stock[self.stock['Date'] > (self.max_date - pd.DateOffset(years = self.training_years)).date()]\n",
    "        \n",
    "        if resample:\n",
    "            stock_history = self.resample(stock_history)\n",
    "        \n",
    "        model.fit(stock_history)\n",
    "        \n",
    "        # Make and predict for next year with future dataframe\n",
    "        future = model.make_future_dataframe(periods = days, freq='D')\n",
    "        future = model.predict(future)\n",
    "        \n",
    "        if days > 0:\n",
    "            # Print the predicted price\n",
    "            print('Predicted Price on {} = ${:.2f}'.format(\n",
    "                future.ix[len(future) - 1, 'ds'].date(), future.ix[len(future) - 1, 'yhat']))\n",
    "\n",
    "            title = '%s Historical and Predicted Stock Price'  % self.symbol\n",
    "        else:\n",
    "            title = '%s Historical and Modeled Stock Price' % self.symbol\n",
    "        \n",
    "        # Set up the plot\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "        # Plot the actual values\n",
    "        ax.plot(stock_history['ds'], stock_history['y'], 'ko-', linewidth = 1.4, alpha = 0.8, ms = 1.8, label = 'Observations')\n",
    "        \n",
    "        # Plot the predicted values\n",
    "        ax.plot(future['ds'], future['yhat'], 'forestgreen',linewidth = 2.4, label = 'Modeled');\n",
    "\n",
    "        # Plot the uncertainty interval as ribbon\n",
    "        ax.fill_between(future['ds'].dt.to_pydatetime(), future['yhat_upper'], future['yhat_lower'], alpha = 0.3, \n",
    "                       facecolor = 'g', edgecolor = 'k', linewidth = 1.4, label = 'Confidence Interval')\n",
    "\n",
    "        # Plot formatting\n",
    "        plt.legend(loc = 2, prop={'size': 10}); plt.xlabel('Date'); plt.ylabel('Price $');\n",
    "        plt.grid(linewidth=0.6, alpha = 0.6)\n",
    "        plt.title(title);\n",
    "        plt.show()\n",
    "        \n",
    "        return model, future\n",
    "      \n",
    "    # Evaluate prediction model for one year\n",
    "    def evaluate_prediction(self, start_date=None, end_date=None, nshares = None):\n",
    "        \n",
    "        # Default start date is one year before end of data\n",
    "        # Default end date is end date of data\n",
    "        if start_date is None:\n",
    "            start_date = self.max_date - pd.DateOffset(years=1)\n",
    "        if end_date is None:\n",
    "            end_date = self.max_date\n",
    "            \n",
    "        start_date, end_date = self.handle_dates(start_date, end_date)\n",
    "        \n",
    "        # Training data starts self.training_years years before start date and goes up to start date\n",
    "        train = self.stock[(self.stock['Date'] < start_date.date()) & \n",
    "                           (self.stock['Date'] > (start_date - pd.DateOffset(years=self.training_years)).date())]\n",
    "        \n",
    "        # Testing data is specified in the range\n",
    "        test = self.stock[(self.stock['Date'] >= start_date.date()) & (self.stock['Date'] <= end_date.date())]\n",
    "        \n",
    "        # Create and train the model\n",
    "        model = self.create_model()\n",
    "        model.fit(train)\n",
    "        \n",
    "        # Make a future dataframe and predictions\n",
    "        future = model.make_future_dataframe(periods = 365, freq='D')\n",
    "        future = model.predict(future)\n",
    "        \n",
    "        # Merge predictions with the known values\n",
    "        test = pd.merge(test, future, on = 'ds', how = 'inner')\n",
    "\n",
    "        train = pd.merge(train, future, on = 'ds', how = 'inner')\n",
    "        \n",
    "        # Calculate the differences between consecutive measurements\n",
    "        test['pred_diff'] = test['yhat'].diff()\n",
    "        test['real_diff'] = test['y'].diff()\n",
    "        \n",
    "        # Correct is when we predicted the correct direction\n",
    "        test['correct'] = (np.sign(test['pred_diff']) == np.sign(test['real_diff'])) * 1\n",
    "        \n",
    "        # Accuracy when we predict increase and decrease\n",
    "        increase_accuracy = 100 * np.mean(test[test['pred_diff'] > 0]['correct'])\n",
    "        decrease_accuracy = 100 * np.mean(test[test['pred_diff'] < 0]['correct'])\n",
    "\n",
    "        # Calculate mean absolute error\n",
    "        test_errors = abs(test['y'] - test['yhat'])\n",
    "        test_mean_error = np.mean(test_errors)\n",
    "\n",
    "        train_errors = abs(train['y'] - train['yhat'])\n",
    "        train_mean_error = np.mean(train_errors)\n",
    "\n",
    "        # Calculate percentage of time actual value within prediction range\n",
    "        test['in_range'] = False\n",
    "\n",
    "        for i in test.index:\n",
    "            if (test.ix[i, 'y'] < test.ix[i, 'yhat_upper']) & (test.ix[i, 'y'] > test.ix[i, 'yhat_lower']):\n",
    "                test.ix[i, 'in_range'] = True\n",
    "\n",
    "        in_range_accuracy = 100 * np.mean(test['in_range'])\n",
    "\n",
    "        if not nshares:\n",
    "\n",
    "            # Date range of predictions\n",
    "            print('\\nPrediction Range: {} to {}.'.format(start_date.date(),\n",
    "                end_date.date()))\n",
    "\n",
    "            # Final prediction vs actual value\n",
    "            print('\\nPredicted price on {} = ${:.2f}.'.format(max(future['ds']).date(), future.ix[len(future) - 1, 'yhat']))\n",
    "            print('Actual price on    {} = ${:.2f}.\\n'.format(max(test['ds']).date(), test.ix[len(test) - 1, 'y']))\n",
    "\n",
    "            print('Average Absolute Error on Training Data = ${:.2f}.'.format(train_mean_error))\n",
    "            print('Average Absolute Error on Testing  Data = ${:.2f}.\\n'.format(test_mean_error))\n",
    "\n",
    "            # Direction accuracy\n",
    "            print('When the model predicted an increase, the price increased {:.2f}% of the time.'.format(increase_accuracy))\n",
    "            print('When the model predicted a  decrease, the price decreased  {:.2f}% of the time.\\n'.format(decrease_accuracy))\n",
    "\n",
    "            print('The actual value was within the {:d}% confidence interval {:.2f}% of the time.'.format(int(100 * model.interval_width), in_range_accuracy))\n",
    "\n",
    "\n",
    "             # Reset the plot\n",
    "            self.reset_plot()\n",
    "            \n",
    "            # Set up the plot\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "            # Plot the actual values\n",
    "            ax.plot(train['ds'], train['y'], 'ko-', linewidth = 1.4, alpha = 0.8, ms = 1.8, label = 'Observations')\n",
    "            ax.plot(test['ds'], test['y'], 'ko-', linewidth = 1.4, alpha = 0.8, ms = 1.8, label = 'Observations')\n",
    "            \n",
    "            # Plot the predicted values\n",
    "            ax.plot(future['ds'], future['yhat'], 'navy', linewidth = 2.4, label = 'Predicted');\n",
    "\n",
    "            # Plot the uncertainty interval as ribbon\n",
    "            ax.fill_between(future['ds'].dt.to_pydatetime(), future['yhat_upper'], future['yhat_lower'], alpha = 0.6, \n",
    "                           facecolor = 'gold', edgecolor = 'k', linewidth = 1.4, label = 'Confidence Interval')\n",
    "\n",
    "            # Put a vertical line at the start of predictions\n",
    "            plt.vlines(x=min(test['ds']).date(), ymin=min(future['yhat_lower']), ymax=max(future['yhat_upper']), colors = 'r',\n",
    "                       linestyles='dashed', label = 'Prediction Start')\n",
    "\n",
    "            # Plot formatting\n",
    "            plt.legend(loc = 2, prop={'size': 8}); plt.xlabel('Date'); plt.ylabel('Price $');\n",
    "            plt.grid(linewidth=0.6, alpha = 0.6)\n",
    "                       \n",
    "            plt.title('{} Model Evaluation from {} to {}.'.format(self.symbol,\n",
    "                start_date.date(), end_date.date()));\n",
    "            plt.show();\n",
    "\n",
    "        \n",
    "        # If a number of shares is specified, play the game\n",
    "        elif nshares:\n",
    "            \n",
    "            # Only playing the stocks when we predict the stock will increase\n",
    "            test_pred_increase = test[test['pred_diff'] > 0]\n",
    "            \n",
    "            test_pred_increase.reset_index(inplace=True)\n",
    "            prediction_profit = []\n",
    "            \n",
    "            # Iterate through all the predictions and calculate profit from playing\n",
    "            for i, correct in enumerate(test_pred_increase['correct']):\n",
    "                \n",
    "                # If we predicted up and the price goes up, we gain the difference\n",
    "                if correct == 1:\n",
    "                    prediction_profit.append(nshares * test_pred_increase.ix[i, 'real_diff'])\n",
    "                # If we predicted up and the price goes down, we lose the difference\n",
    "                else:\n",
    "                    prediction_profit.append(nshares * test_pred_increase.ix[i, 'real_diff'])\n",
    "            \n",
    "            test_pred_increase['pred_profit'] = prediction_profit\n",
    "            \n",
    "            # Put the profit into the test dataframe\n",
    "            test = pd.merge(test, test_pred_increase[['ds', 'pred_profit']], on = 'ds', how = 'left')\n",
    "            test.ix[0, 'pred_profit'] = 0\n",
    "        \n",
    "            # Profit for either method at all dates\n",
    "            test['pred_profit'] = test['pred_profit'].cumsum().ffill()\n",
    "            test['hold_profit'] = nshares * (test['y'] - float(test.ix[0, 'y']))\n",
    "            \n",
    "            # Display information\n",
    "            print('You played the stock market in {} from {} to {} with {} shares.\\n'.format(\n",
    "                self.symbol, start_date.date(), end_date.date(), nshares))\n",
    "            \n",
    "            print('When the model predicted an increase, the price increased {:.2f}% of the time.'.format(increase_accuracy))\n",
    "            print('When the model predicted a  decrease, the price decreased  {:.2f}% of the time.\\n'.format(decrease_accuracy))\n",
    "\n",
    "            # Display some friendly information about the perils of playing the stock market\n",
    "            print('The total profit using the Prophet model = ${:.2f}.'.format(np.sum(prediction_profit)))\n",
    "            print('The Buy and Hold strategy profit =         ${:.2f}.'.format(float(test.ix[len(test) - 1, 'hold_profit'])))\n",
    "            print('\\nThanks for playing the stock market!\\n')\n",
    "            \n",
    "           \n",
    "            \n",
    "            # Plot the predicted and actual profits over time\n",
    "            self.reset_plot()\n",
    "            \n",
    "            # Final profit and final smart used for locating text\n",
    "            final_profit = test.ix[len(test) - 1, 'pred_profit']\n",
    "            final_smart = test.ix[len(test) - 1, 'hold_profit']\n",
    "\n",
    "            # text location\n",
    "            last_date = test.ix[len(test) - 1, 'ds']\n",
    "            text_location = (last_date - pd.DateOffset(months = 1)).date()\n",
    "\n",
    "            plt.style.use('dark_background')\n",
    "\n",
    "            # Plot smart profits\n",
    "            plt.plot(test['ds'], test['hold_profit'], 'b',\n",
    "                     linewidth = 1.8, label = 'Buy and Hold Strategy') \n",
    "\n",
    "            # Plot prediction profits\n",
    "            plt.plot(test['ds'], test['pred_profit'], \n",
    "                     color = 'g' if final_profit > 0 else 'r',\n",
    "                     linewidth = 1.8, label = 'Prediction Strategy')\n",
    "\n",
    "            # Display final values on graph\n",
    "            plt.text(x = text_location, \n",
    "                     y =  final_profit + (final_profit / 40),\n",
    "                     s = '$%d' % final_profit,\n",
    "                    color = 'g' if final_profit > 0 else 'r',\n",
    "                    size = 18)\n",
    "            \n",
    "            plt.text(x = text_location, \n",
    "                     y =  final_smart + (final_smart / 40),\n",
    "                     s = '$%d' % final_smart,\n",
    "                    color = 'g' if final_smart > 0 else 'r',\n",
    "                    size = 18);\n",
    "\n",
    "            # Plot formatting\n",
    "            plt.ylabel('Profit  (US $)'); plt.xlabel('Date'); \n",
    "            plt.title('Predicted versus Buy and Hold Profits');\n",
    "            plt.legend(loc = 2, prop={'size': 10});\n",
    "            plt.grid(alpha=0.2); \n",
    "            plt.show()\n",
    "        \n",
    "    def retrieve_google_trends(self, search, date_range):\n",
    "        \n",
    "        # Set up the trend fetching object\n",
    "        pytrends = TrendReq(hl='en-US', tz=360)\n",
    "        kw_list = [search]\n",
    "\n",
    "        try:\n",
    "        \n",
    "            # Create the search object\n",
    "            pytrends.build_payload(kw_list, cat=0, timeframe=date_range[0], geo='', gprop='news')\n",
    "            \n",
    "            # Retrieve the interest over time\n",
    "            trends = pytrends.interest_over_time()\n",
    "\n",
    "            related_queries = pytrends.related_queries()\n",
    "\n",
    "        except Exception as e:\n",
    "            print('\\nGoogle Search Trend retrieval failed.')\n",
    "            print(e)\n",
    "            return\n",
    "        \n",
    "        return trends, related_queries\n",
    "        \n",
    "    def changepoint_date_analysis(self, search=None):\n",
    "        self.reset_plot()\n",
    "\n",
    "        model = self.create_model()\n",
    "        \n",
    "        # Use past self.training_years years of data\n",
    "        train = self.stock[self.stock['Date'] > (self.max_date - pd.DateOffset(years = self.training_years)).date()]\n",
    "        model.fit(train)\n",
    "        \n",
    "        # Predictions of the training data (no future periods)\n",
    "        future = model.make_future_dataframe(periods=0, freq='D')\n",
    "        future = model.predict(future)\n",
    "    \n",
    "        train = pd.merge(train, future[['ds', 'yhat']], on = 'ds', how = 'inner')\n",
    "        \n",
    "        changepoints = model.changepoints\n",
    "        train = train.reset_index(drop=True)\n",
    "        \n",
    "        # Create dataframe of only changepoints\n",
    "        change_indices = []\n",
    "        for changepoint in (changepoints):\n",
    "            change_indices.append(train[train['ds'] == changepoint.date()].index[0])\n",
    "        \n",
    "        c_data = train.ix[change_indices, :]\n",
    "        deltas = model.params['delta'][0]\n",
    "        \n",
    "        c_data['delta'] = deltas\n",
    "        c_data['abs_delta'] = abs(c_data['delta'])\n",
    "        \n",
    "        # Sort the values by maximum change\n",
    "        c_data = c_data.sort_values(by='abs_delta', ascending=False)\n",
    "\n",
    "        # Limit to 10 largest changepoints\n",
    "        c_data = c_data[:10]\n",
    "\n",
    "        # Separate into negative and positive changepoints\n",
    "        cpos_data = c_data[c_data['delta'] > 0]\n",
    "        cneg_data = c_data[c_data['delta'] < 0]\n",
    "\n",
    "        # Changepoints and data\n",
    "        if not search:\n",
    "        \n",
    "            print('\\nChangepoints sorted by slope rate of change (2nd derivative):\\n')\n",
    "            print(c_data.ix[:, ['Date', 'Adj. Close', 'delta']][:5])\n",
    "\n",
    "            # Line plot showing actual values, estimated values, and changepoints\n",
    "            self.reset_plot()\n",
    "            \n",
    "            # Set up line plot \n",
    "            plt.plot(train['ds'], train['y'], 'ko', ms = 4, label = 'Stock Price')\n",
    "            plt.plot(future['ds'], future['yhat'], color = 'navy', linewidth = 2.0, label = 'Modeled')\n",
    "            \n",
    "            # Changepoints as vertical lines\n",
    "            plt.vlines(cpos_data['ds'].dt.to_pydatetime(), ymin = min(train['y']), ymax = max(train['y']), \n",
    "                       linestyles='dashed', color = 'r', \n",
    "                       linewidth= 1.2, label='Negative Changepoints')\n",
    "\n",
    "            plt.vlines(cneg_data['ds'].dt.to_pydatetime(), ymin = min(train['y']), ymax = max(train['y']), \n",
    "                       linestyles='dashed', color = 'darkgreen', \n",
    "                       linewidth= 1.2, label='Positive Changepoints')\n",
    "\n",
    "            plt.legend(prop={'size':10});\n",
    "            plt.xlabel('Date'); plt.ylabel('Price ($)'); plt.title('Stock Price with Changepoints')\n",
    "            plt.show()\n",
    "        \n",
    "        # Search for search term in google news\n",
    "        # Show related queries, rising related queries\n",
    "        # Graph changepoints, search frequency, stock price\n",
    "        if search:\n",
    "            date_range = ['%s %s' % (str(min(train['Date']).date()), str(max(train['Date']).date()))]\n",
    "\n",
    "            # Get the Google Trends for specified terms and join to training dataframe\n",
    "            trends, related_queries = self.retrieve_google_trends(search, date_range)\n",
    "\n",
    "            if (trends is None)  or (related_queries is None):\n",
    "                print('No search trends found for %s' % search)\n",
    "                return\n",
    "\n",
    "            print('\\n Top Related Queries: \\n')\n",
    "            print(related_queries[search]['top'].head())\n",
    "\n",
    "            print('\\n Rising Related Queries: \\n')\n",
    "            print(related_queries[search]['rising'].head())\n",
    "\n",
    "            # Upsample the data for joining with training data\n",
    "            trends = trends.resample('D')\n",
    "\n",
    "            trends = trends.reset_index(level=0)\n",
    "            trends = trends.rename(columns={'date': 'ds', search: 'freq'})\n",
    "\n",
    "            # Interpolate the frequency\n",
    "            trends['freq'] = trends['freq'].interpolate()\n",
    "\n",
    "            # Merge with the training data\n",
    "            train = pd.merge(train, trends, on = 'ds', how = 'inner')\n",
    "\n",
    "            # Normalize values\n",
    "            train['y_norm'] = train['y'] / max(train['y'])\n",
    "            train['freq_norm'] = train['freq'] / max(train['freq'])\n",
    "            \n",
    "            self.reset_plot()\n",
    "\n",
    "            # Plot the normalized stock price and normalize search frequency\n",
    "            plt.plot(train['ds'], train['y_norm'], 'k-', label = 'Stock Price')\n",
    "            plt.plot(train['ds'], train['freq_norm'], color='goldenrod', label = 'Search Frequency')\n",
    "\n",
    "            # Changepoints as vertical lines\n",
    "            plt.vlines(cpos_data['ds'].dt.to_pydatetime(), ymin = 0, ymax = 1, \n",
    "                       linestyles='dashed', color = 'r', \n",
    "                       linewidth= 1.2, label='Negative Changepoints')\n",
    "\n",
    "            plt.vlines(cneg_data['ds'].dt.to_pydatetime(), ymin = 0, ymax = 1, \n",
    "                       linestyles='dashed', color = 'darkgreen', \n",
    "                       linewidth= 1.2, label='Positive Changepoints')\n",
    "\n",
    "            # Plot formatting\n",
    "            plt.legend(prop={'size': 10})\n",
    "            plt.xlabel('Date'); plt.ylabel('Normalized Values'); plt.title('%s Stock Price and Search Frequency for %s' % (self.symbol, search))\n",
    "            plt.show()\n",
    "        \n",
    "    # Predict the future price for a given range of days\n",
    "    def predict_future(self, days=30):\n",
    "        \n",
    "        # Use past self.training_years years for training\n",
    "        train = self.stock[self.stock['Date'] > (max(self.stock['Date']) - pd.DateOffset(years=self.training_years)).date()]\n",
    "        \n",
    "        model = self.create_model()\n",
    "        \n",
    "        model.fit(train)\n",
    "        \n",
    "        # Future dataframe with specified number of days to predict\n",
    "        future = model.make_future_dataframe(periods=days, freq='D')\n",
    "        future = model.predict(future)\n",
    "        \n",
    "        # Only concerned with future dates\n",
    "        future = future[future['ds'] >= max(self.stock['Date']).date()]\n",
    "        \n",
    "        # Remove the weekends\n",
    "        future = self.remove_weekends(future)\n",
    "        \n",
    "        # Calculate whether increase or not\n",
    "        future['diff'] = future['yhat'].diff()\n",
    "    \n",
    "        future = future.dropna()\n",
    "\n",
    "        # Find the prediction direction and create separate dataframes\n",
    "        future['direction'] = (future['diff'] > 0) * 1\n",
    "        \n",
    "        # Rename the columns for presentation\n",
    "        future = future.rename(columns={'ds': 'Date', 'yhat': 'estimate', 'diff': 'change', \n",
    "                                        'yhat_upper': 'upper', 'yhat_lower': 'lower'})\n",
    "        \n",
    "        future_increase = future[future['direction'] == 1]\n",
    "        future_decrease = future[future['direction'] == 0]\n",
    "        \n",
    "        # Print out the dates\n",
    "        print('\\nPredicted Increase: \\n')\n",
    "        print(future_increase[['Date', 'estimate', 'change', 'upper', 'lower']])\n",
    "        \n",
    "        print('\\nPredicted Decrease: \\n')\n",
    "        print(future_decrease[['Date', 'estimate', 'change', 'upper', 'lower']])\n",
    "        \n",
    "        self.reset_plot()\n",
    "        \n",
    "        # Set up plot\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        matplotlib.rcParams['axes.labelsize'] = 10\n",
    "        matplotlib.rcParams['xtick.labelsize'] = 8\n",
    "        matplotlib.rcParams['ytick.labelsize'] = 8\n",
    "        matplotlib.rcParams['axes.titlesize'] = 12\n",
    "        \n",
    "        # Plot the predictions and indicate if increase or decrease\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "        # Plot the estimates\n",
    "        ax.plot(future_increase['Date'], future_increase['estimate'], 'g^', ms = 12, label = 'Pred. Increase')\n",
    "        ax.plot(future_decrease['Date'], future_decrease['estimate'], 'rv', ms = 12, label = 'Pred. Decrease')\n",
    "\n",
    "        # Plot errorbars\n",
    "        ax.errorbar(future['Date'].dt.to_pydatetime(), future['estimate'], \n",
    "                    yerr = future['upper'] - future['lower'], \n",
    "                    capthick=1.4, color = 'k',linewidth = 2,\n",
    "                   ecolor='darkblue', capsize = 4, elinewidth = 1, label = 'Pred with Range')\n",
    "\n",
    "        # Plot formatting\n",
    "        plt.legend(loc = 2, prop={'size': 10});\n",
    "        plt.xticks(rotation = '45')\n",
    "        plt.ylabel('Predicted Stock Price (US $)');\n",
    "        plt.xlabel('Date'); plt.title('Predictions for %s' % self.symbol);\n",
    "        plt.show()\n",
    "        \n",
    "    def changepoint_prior_validation(self, start_date=None, end_date=None,changepoint_priors = [0.001, 0.05, 0.1, 0.2]):\n",
    "\n",
    "\n",
    "        # Default start date is two years before end of data\n",
    "        # Default end date is one year before end of data\n",
    "        if start_date is None:\n",
    "            start_date = self.max_date - pd.DateOffset(years=2)\n",
    "        if end_date is None:\n",
    "            end_date = self.max_date - pd.DateOffset(years=1)\n",
    "            \n",
    "        # Convert to pandas datetime for indexing dataframe\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        \n",
    "        start_date, end_date = self.handle_dates(start_date, end_date)\n",
    "                               \n",
    "        # Select self.training_years number of years\n",
    "        train = self.stock[(self.stock['Date'] > (start_date - pd.DateOffset(years=self.training_years)).date()) & \n",
    "        (self.stock['Date'] < start_date.date())]\n",
    "        \n",
    "        # Testing data is specified by range\n",
    "        test = self.stock[(self.stock['Date'] >= start_date.date()) & (self.stock['Date'] <= end_date.date())]\n",
    "\n",
    "        eval_days = (max(test['Date']).date() - min(test['Date']).date()).days\n",
    "        \n",
    "        results = pd.DataFrame(0, index = list(range(len(changepoint_priors))), \n",
    "            columns = ['cps', 'train_err', 'train_range', 'test_err', 'test_range'])\n",
    "\n",
    "        print('\\nValidation Range {} to {}.\\n'.format(min(test['Date']).date(),\n",
    "            max(test['Date']).date()))\n",
    "            \n",
    "        \n",
    "        # Iterate through all the changepoints and make models\n",
    "        for i, prior in enumerate(changepoint_priors):\n",
    "            results.ix[i, 'cps'] = prior\n",
    "            \n",
    "            # Select the changepoint\n",
    "            self.changepoint_prior_scale = prior\n",
    "            \n",
    "            # Create and train a model with the specified cps\n",
    "            model = self.create_model()\n",
    "            model.fit(train)\n",
    "            future = model.make_future_dataframe(periods=eval_days, freq='D')\n",
    "                \n",
    "            future = model.predict(future)\n",
    "            \n",
    "            # Training results and metrics\n",
    "            train_results = pd.merge(train, future[['ds', 'yhat', 'yhat_upper', 'yhat_lower']], on = 'ds', how = 'inner')\n",
    "            avg_train_error = np.mean(abs(train_results['y'] - train_results['yhat']))\n",
    "            avg_train_uncertainty = np.mean(abs(train_results['yhat_upper'] - train_results['yhat_lower']))\n",
    "            \n",
    "            results.ix[i, 'train_err'] = avg_train_error\n",
    "            results.ix[i, 'train_range'] = avg_train_uncertainty\n",
    "            \n",
    "            # Testing results and metrics\n",
    "            test_results = pd.merge(test, future[['ds', 'yhat', 'yhat_upper', 'yhat_lower']], on = 'ds', how = 'inner')\n",
    "            avg_test_error = np.mean(abs(test_results['y'] - test_results['yhat']))\n",
    "            avg_test_uncertainty = np.mean(abs(test_results['yhat_upper'] - test_results['yhat_lower']))\n",
    "            \n",
    "            results.ix[i, 'test_err'] = avg_test_error\n",
    "            results.ix[i, 'test_range'] = avg_test_uncertainty\n",
    "\n",
    "        print(results)\n",
    "\n",
    "\n",
    "        \n",
    "        # Plot of training and testing average errors\n",
    "        self.reset_plot()\n",
    "        \n",
    "        plt.plot(results['cps'], results['train_err'], 'bo-', ms = 8, label = 'Train Error')\n",
    "        plt.plot(results['cps'], results['test_err'], 'r*-', ms = 8, label = 'Test Error')\n",
    "        plt.xlabel('Changepoint Prior Scale'); plt.ylabel('Avg. Absolute Error ($)');\n",
    "        plt.title('Training and Testing Curves as Function of CPS')\n",
    "        plt.grid(color='k', alpha=0.3)\n",
    "        plt.xticks(results['cps'], results['cps'])\n",
    "        plt.legend(prop={'size':10})\n",
    "        plt.show();\n",
    "        \n",
    "        # Plot of training and testing average uncertainty\n",
    "        self.reset_plot()\n",
    "\n",
    "        plt.plot(results['cps'], results['train_range'], 'bo-', ms = 8, label = 'Train Range')\n",
    "        plt.plot(results['cps'], results['test_range'], 'r*-', ms = 8, label = 'Test Range')\n",
    "        plt.xlabel('Changepoint Prior Scale'); plt.ylabel('Avg. Uncertainty ($)');\n",
    "        plt.title('Uncertainty in Estimate as Function of CPS')\n",
    "        plt.grid(color='k', alpha=0.3)\n",
    "        plt.xticks(results['cps'], results['cps'])\n",
    "        plt.legend(prop={'size':10})\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Applications/anaconda3/lib/python3.7/site-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Applications/anaconda3/lib/python3.7/site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /Applications/anaconda3/lib/python3.7/site-packages (from keras) (5.3)\n",
      "Requirement already satisfied: h5py in /Applications/anaconda3/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.7/site-packages (from h5py->keras) (1.14.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (165.1 MB)\n",
      "\u001b[K     || 165.1 MB 14 kB/s s eta 0:00:01  |                              | 5.9 MB 4.4 MB/s eta 0:00:37     |                     | 53.8 MB 18.7 MB/s eta 0:00:06     |                    | 59.2 MB 18.0 MB/s eta 0:00:06     |                   | 63.4 MB 18.0 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     || 57 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     || 459 kB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     || 3.3 MB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     || 127 kB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     || 42 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     || 6.8 MB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.13.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     || 1.3 MB 35.7 MB/s eta 0:00:01     |                 | 573 kB 35.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     || 65 kB 8.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     || 114 kB 44.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     || 779 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (46.0.0.post20200309)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3-py3-none-any.whl (94 kB)\n",
      "\u001b[K     || 94 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     || 47 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     || 155 kB 38.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Applications/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.5.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     || 77 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     || 147 kB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.2.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=cbf51b7c5e9016a1cd573ac0e37dbecbc253c82dc16b2043dcc07a9da3cf00b3\n",
      "  Stored in directory: /Users/scorpion/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: google-pasta, tensorflow-estimator, grpcio, absl-py, keras-preprocessing, gast, astunparse, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, tensorboard-plugin-wit, markdown, tensorboard, termcolor, opt-einsum, tensorflow\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Downloading fastai-2.0.15-py3-none-any.whl (185 kB)\n",
      "\u001b[K     || 185 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (7.0.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.6.0-cp37-none-macosx_10_9_x86_64.whl (97.4 MB)\n",
      "\u001b[K     || 97.4 MB 59 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision>=0.7\n",
      "  Downloading torchvision-0.7.0-cp37-cp37m-macosx_10_9_x86_64.whl (387 kB)\n",
      "\u001b[K     || 387 kB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (1.1.3)\n",
      "Requirement already satisfied: pyyaml in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (5.3)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (2.22.0)\n",
      "Collecting fastcore>=1.0.5\n",
      "  Downloading fastcore-1.0.21-py3-none-any.whl (41 kB)\n",
      "\u001b[K     || 41 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (0.22.1)\n",
      "Requirement already satisfied: pip in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (20.0.2)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (20.1)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (1.4.1)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-macosx_10_9_x86_64.whl (10.0 MB)\n",
      "\u001b[K     || 10.0 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.7/site-packages (from fastai) (3.1.3)\n",
      "Requirement already satisfied: future in /Applications/anaconda3/lib/python3.7/site-packages (from torch>=1.6.0->fastai) (0.18.2)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.7/site-packages (from torch>=1.6.0->fastai) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas->fastai) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas->fastai) (2019.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->fastai) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->fastai) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->fastai) (1.25.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /Applications/anaconda3/lib/python3.7/site-packages (from scikit-learn->fastai) (0.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from packaging->fastai) (2.4.6)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.7/site-packages (from packaging->fastai) (1.14.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/anaconda3/lib/python3.7/site-packages (from spacy->fastai) (4.42.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (34 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (211 kB)\n",
      "\u001b[K     || 211 kB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.7/site-packages (from spacy->fastai) (46.0.0.post20200309)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.0 MB)\n",
      "\u001b[K     || 4.0 MB 24.1 MB/s eta 0:00:01     |         | 2.9 MB 24.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp37-cp37m-macosx_10_6_intel.whl (54 kB)\n",
      "\u001b[K     || 54 kB 5.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (182 kB)\n",
      "\u001b[K     || 182 kB 29.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp37-cp37m-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[K     || 2.1 MB 25.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.7/site-packages (from matplotlib->fastai) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.7/site-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Applications/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai) (2.2.0)\n",
      "Installing collected packages: torch, torchvision, fastcore, fastprogress, murmurhash, cymem, preshed, plac, wasabi, catalogue, blis, srsly, thinc, spacy, fastai\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 fastai-2.0.15 fastcore-1.0.21 fastprogress-1.0.0 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 torch-1.6.0 torchvision-0.7.0 wasabi-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quandl\n",
    "\n",
    "#to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#setting figure size\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "#importing required libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "#from fastai.structured import add_datepart\n",
    "\n",
    "#to get stock OHCL data\n",
    "#from stocker import Stocker\n",
    "\n",
    "quandl.ApiConfig.api_key = 'sweXuuAZ8xRHtz-LayUM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10 years data\n",
    "period = 365 * 10\n",
    "# period of 10 years from 12-31-2017 backwards go back to 01-04-2008\n",
    "date_range = ('09-01-2019', '09-01-2020')\n",
    "max_stocker = '10-07-2020'\n",
    "min_date = datetime.datetime.strptime(date_range[0], \"%m-%d-%Y\")\n",
    "max_date = datetime.datetime.strptime(max_stocker, \"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 years data\n",
    "period = 365 * 10\n",
    "# period of 10 years from 12-31-2017 backwards go back to 01-04-2008\n",
    "date_range = ('01-01-2008', '12-31-2017')\n",
    "max_stocker = '03-27-2018'\n",
    "min_date = datetime.datetime.strptime(date_range[0], \"%m-%d-%Y\")\n",
    "max_date = datetime.datetime.strptime(max_stocker, \"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "best100 = ['ABBC', 'ADA', 'ALGO', 'ATOM', 'BAT', 'BCD', 'BCH', 'BCN', 'BNB', 'BNT', 'BSV', 'BTC', 'BTG', 'BTM', 'BTT', 'CCC', 'CENNZ', 'CRO', 'DAT', 'DASH', 'DCR', 'DGB', 'DOGE', 'EKT', 'ELA', 'ENJ', 'EON', 'EOS', 'ETC', 'ETH', 'GAP', 'GXC', 'HBAR', 'HC', 'HEDG', 'HOT'\n",
    ",'HT', 'ICX', 'INB', 'INO', 'IOST', 'KBC', 'KCS', 'KIN', 'KMD', 'KNC', 'LEO', 'LINK', 'LSK', 'LTC', 'MCO', 'MIN', 'MIOTA', 'MKR', 'MONA', 'NANO',\n",
    "'NEO', 'NEXO', 'NGC', 'OMG', 'ONE', 'ONT', 'PAX' ,  'QTUM', 'REN', 'REP', 'RVN', 'SC', 'SEELE', 'SNX', 'SXP', 'THETA', 'TRX', 'TUSD', 'USDC', \n",
    "'USDT', 'VEE', 'VET', 'VSYS', 'WAVES', 'WTC', 'XEM', 'XIN', 'XLM', 'XMR', 'XRP', 'XTZ', 'XVG', 'ZB', 'ZEC', 'ZRX']\n",
    "         \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n",
      "Error Retrieving Data.\n",
      "(Status 404) (Quandl Error QECx02) You have submitted an incorrect Quandl code. Please check your Quandl codes and try again.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bc3b8970c461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtickerobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtickerobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStocker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-54298a258b6c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ticker, exchange)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adj. Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mstock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Daily Change'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adj. Close'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adj. Open'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Data assigned as class attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;34mf\"dimension must be <= 2: {right.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             )\n\u001b[0;32m--> 500\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# GH17901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mEvaluate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcomparison\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[0;34m(left, right, op, str_rep)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m#  In this case we do not fall back to the masked op, as that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m#  will handle complex numbers incorrectly, see GH#32047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mop_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_str_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop_str\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0muse_numexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "tickers = best100[:50]\n",
    "tickerobjs = {} \n",
    "for ticker in tickers:\n",
    "    tickerobjs[ticker] = (Stocker(ticker=ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'tensorflow_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3d00d838479b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'tensorflow_backend'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement MySQLdb (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for MySQLdb\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'close' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-53dca7c961e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'SELECT * FROM ico_price_daily_summaries where currency =\"USD\";'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'close' is not defined"
     ]
    }
   ],
   "source": [
    "ieo = pd.DataFrame()\n",
    "price = pd.DataFrame()\n",
    "grades = pd.DataFrame()\n",
    "\n",
    "import mysql.connector\n",
    "conn = mysql.connector.connect(host='tokenmetrics-restored-27-05.cxuzrhvtziar.us-east-1.rds.amazonaws.com', user='admin', passwd='WiG8Rled2cTvZ5JibJui',db='tokenmetrics')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = '''SELECT * FROM icos;'''\n",
    "data = pd.read_sql_query(query, conn)\n",
    "ieo = pd.concat([ieo,data])\n",
    "\n",
    "query = 'SELECT * FROM ico_price_daily_summaries where currency =\"USD\";'\n",
    "data = pd.read_sql_query(query, conn)\n",
    "close = pd.concat([close,data])\n",
    "\n",
    " \n",
    "    \n",
    "conn.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieo = ieo.drop(ieo[ieo['id'] == 3245].index)\n",
    "ieo = ieo[:250] \n",
    " \n",
    "ieo = pd.merge(ieo,price[['ico_id','ico_symbol']], left_on='id',right_on='ico_id',how='inner')\n",
    "ieo = ieo.drop_duplicates('id')\n",
    "close = ieo[['ico_symbol','close']]\n",
    "close = close.sort_values('ico_symbol')\n",
    "\n",
    "ieos = ieo['id'][:250]\n",
    "df = close[close['ico_id'].isin(ieos)]\n",
    "df = df.drop_duplicates(subset=['ico_symbol','date']).sort_values(by='date')\n",
    "\n",
    "\n",
    "temp = temp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['close'].groupby([df['date'],df['ico_symbol']]).mean()\n",
    "temp = temp.unstack().sort_index()\n",
    "temp.index = pd.to_datetime(temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_data(coin_symbol, config_file='/Users/scorpion/Downloads/sql_crendentials.ini'):\n",
    "    '''\n",
    "    Retrieve the price data corresponding to coin_name from the Token Metrics database.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    coin_symbol: the symbol of the token\n",
    "    config_file: ini file which contains Token Metrics db credentials\n",
    "\n",
    "    return\n",
    "    -------\n",
    "    The data frame containing all the price data for the coin.\n",
    "    '''\n",
    "    config = ConfigParser()\n",
    "    config.read(config_file)\n",
    "    user = config['SQL']['user']\n",
    "    password = config['SQL']['password']\n",
    "\n",
    "    import mysql.connector\n",
    "    token_metrics_db = mysql.connector.connect(user='mluser', password='ml@t0kenMetrics',\n",
    "                                               host='tokenmetrics-restored-27-05.cxuzrhvtziar.us-east-1.rds.amazonaws.com',\n",
    "                                               database='tokenmetrics')\n",
    "    crypto_prices = pd.read_sql_query(\"SELECT * FROM ico_price_daily_summaries WHERE currency = 'USD'\", token_metrics_db)\n",
    "    token_metrics_db.close()\n",
    "    prices = crypto_prices[crypto_prices['ico_symbol'] == coin_symbol].copy()\n",
    "    prices.loc[:, 'date'] = pd.to_datetime(prices.date)\n",
    "    if len(prices) == 0:\n",
    "        print('No price data for this coin.')\n",
    "        return None\n",
    "    print('Price data retrieved.')\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coin_symbol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-eb9cf9cc5c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mget_price_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_price_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoin_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extracting sentiment of tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coin_symbol' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing price data for the coin\n",
    "##########################\n",
    "get_price_info = True\n",
    "##########################\n",
    "\n",
    "if get_price_info:\n",
    "    prices = get_price_data(coin_symbol)\n",
    "    \n",
    "# Extracting sentiment of tweets\n",
    "#sentiment_data = get_sentiment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import files\n",
    "df=pd.DataFrame(sentiment_data)\n",
    "df.to_csv('sentiment_data.csv')\n",
    " \n",
    "save = pd.DataFrame(sentiment_data, columns = ['username','date','day','hashtags','mentions','retweets','favorites','text','blob_polarity','blob_sentiment','vader_polarity','vader_sentiment','sentiment_score\tsentiment']) \n",
    "save.to_csv('./modified.csv',index=False,header=False) \n",
    "\n",
    "import files\n",
    "df=pd.DataFrame(prices)\n",
    "df.to_csv('price_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enumerate(prices.items()):\n",
    "    sentiment_df = sentiment_data[]\n",
    "    forward_merged = pd.merge_asof(df, sentiment_df, left_index=True, right_index=True, direction='backward')\n",
    "    stocks_data[id] = forward_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(sentiment_data, columns=['sentiment']) #.pct_change()\n",
    "test_one =  pd.DataFrame(prices, columns=['close'])\n",
    "test = test.reset_index()\n",
    "test_one= test_one.reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
