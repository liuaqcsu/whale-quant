{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7_YxJkbdZa3"
   },
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3027,
     "status": "ok",
     "timestamp": 1595345570041,
     "user": {
      "displayName": "Paul Doan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_PRq6r_cx6Muc12StzP-KUbK4iRJP16OzW0ym=s64",
      "userId": "09758343323255005129"
     },
     "user_tz": 240
    },
    "id": "Yfx7OlNJQUFv",
    "outputId": "de951abc-d03c-4747-88ea-cdd8ac4de515"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#! pip install GetOldTweets3\n",
    "import GetOldTweets3 as got3\n",
    "import datetime\n",
    "import time as t\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input\n",
    "Please input your query & number of tweets here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ENTER THE QUERYWORDS as a string, no case sensitivity, can use boolean operations\n",
    "name_of_coin = \"Ethereum\"\n",
    "coin_symbol = \"ETH\"\n",
    "#####################################################################################\n",
    "\n",
    "nTweets = 100000\n",
    "date_limit = datetime.date(2020, 1 ,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9qQi96c681d"
   },
   "source": [
    "## Gathering the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1595345579030,
     "user": {
      "displayName": "Paul Doan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_PRq6r_cx6Muc12StzP-KUbK4iRJP16OzW0ym=s64",
      "userId": "09758343323255005129"
     },
     "user_tz": 240
    },
    "id": "eV5ZxfipQUHL"
   },
   "outputs": [],
   "source": [
    "def gathering_data(name_of_coin, coin_symbol, nTweets, date_limit):\n",
    "  \n",
    "    querywords = \"{0} Crypto OR {0} Cryptocurrency OR {0} Coin OR {0} Token OR {1} Crypto \"\\\n",
    "                 \"OR {1} Cryptocurrency OR {1} Coin OR {1} Token OR ${1}\".format(name_of_coin, coin_symbol)\n",
    "    \n",
    "    batch_number = nTweets // 7000 + 1\n",
    "    tweets_per_batch = int(nTweets / batch_number)\n",
    "    date = []\n",
    "    username = []\n",
    "    text = []\n",
    "    hashtags = []\n",
    "    retweets = []\n",
    "    favorites = []\n",
    "    mentions = []\n",
    "    to = []\n",
    "    \n",
    "    upper_date = datetime.date.today() + datetime.timedelta(days=1)\n",
    "    print(f'Date of first scraped tweet: {datetime.date.today()}')\n",
    "    print(f\"Maximum number of batches: {batch_number}\")\n",
    "    print('Starting scraping tweets...\\n')\n",
    "    print('-'*40)\n",
    "\n",
    "    for i in range(batch_number):\n",
    "        print(\"Scraping batch\", i + 1, \"...\")\n",
    "        tic = t.time()\n",
    "    \n",
    "        #Using GOT to gather the tweets\n",
    "        tweetCriteria = got3.manager.TweetCriteria().setQuerySearch(querywords).setMaxTweets(tweets_per_batch).\\\n",
    "                        setUntil(upper_date.strftime(\"%Y-%m-%d\")).setLang(\"en\").setSince(date_limit.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "        try:\n",
    "            get_tweet = got3.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    \n",
    "        except:\n",
    "            print(\"sleeping for 10 minutes\\n\")\n",
    "            t.sleep(10 * 60)\n",
    "            continue \n",
    "            \n",
    "        print(len(get_tweet), \"of\", tweets_per_batch, \"tweets downloaded in \", round(t.time() - tic,2) , \"s\")\n",
    "\n",
    "    \n",
    "        #Updating the lists that will serve to create the DF\n",
    "        for tweet in get_tweet:\n",
    "            date.append(tweet.date)\n",
    "            username.append(tweet.username)\n",
    "            text.append(tweet.text)\n",
    "            hashtags.append(tweet.hashtags)\n",
    "            retweets.append(tweet.retweets)\n",
    "            favorites.append(tweet.favorites)\n",
    "            mentions.append(tweet.mentions)\n",
    "            to.append(tweet.to)\n",
    "            \n",
    "        if len(get_tweet) == 0:\n",
    "            break\n",
    "        \n",
    "        print(f'Updated date of last tweet: {date[-1]}')\n",
    "        #Modifying the upper_date for later requests\n",
    "        upper_date = date[-1].date()\n",
    "        if upper_date <= date_limit:\n",
    "            print(\"Limit date reached.\")\n",
    "            break\n",
    "        \n",
    "        if i != batch_number - 1:\n",
    "            if len(get_tweet) >= 5000:  \n",
    "                print(\"sleeping for 7 minutes...\\n\")\n",
    "                t.sleep(7 * 60)\n",
    "            else:\n",
    "                print(\"sleeping for 2 minute...\\n\")\n",
    "                t.sleep(2 * 60)\n",
    "        \n",
    "        \n",
    "    \n",
    "    tweets = pd.DataFrame({\"date\":date,\"username\":username,\"text\":text,\\\n",
    "                      \"hashtags\":hashtags,\"mentions\":mentions,\"retweets\":retweets,\"favorites\":favorites,\"to\":to})\n",
    "\n",
    "    \n",
    "    filename = '../Data/Unprocessed_data/' + re.sub(' ', '_', name_of_coin) + '_' + str(len(tweets)) + \".csv\"\n",
    " \n",
    "    return tweets, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2358996,
     "status": "ok",
     "timestamp": 1595349013514,
     "user": {
      "displayName": "Paul Doan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh_PRq6r_cx6Muc12StzP-KUbK4iRJP16OzW0ym=s64",
      "userId": "09758343323255005129"
     },
     "user_tz": 240
    },
    "id": "P1uN_G4mxaFP",
    "outputId": "81160e23-bba9-413c-c62c-17e9c228e1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of first scraped tweet: 2020-08-24\n",
      "Maximum number of batches: 15\n",
      "Starting scraping tweets...\n",
      "\n",
      "----------------------------------------\n",
      "Scraping batch 1 ...\n",
      "6666 of 6666 tweets downloaded in  304.75 s\n",
      "Updated date of last tweet: 2020-08-09 19:42:05+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 2 ...\n",
      "6666 of 6666 tweets downloaded in  307.94 s\n",
      "Updated date of last tweet: 2020-07-26 22:45:29+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 3 ...\n",
      "6666 of 6666 tweets downloaded in  318.68 s\n",
      "Updated date of last tweet: 2020-07-10 13:10:03+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 4 ...\n",
      "6666 of 6666 tweets downloaded in  332.15 s\n",
      "Updated date of last tweet: 2020-06-22 20:37:52+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 5 ...\n",
      "6666 of 6666 tweets downloaded in  335.8 s\n",
      "Updated date of last tweet: 2020-06-01 16:46:10+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 6 ...\n",
      "3683 of 6666 tweets downloaded in  177.82 s\n",
      "Updated date of last tweet: 2020-05-19 23:40:34+00:00\n",
      "sleeping for 2 minute...\n",
      "\n",
      "Scraping batch 7 ...\n",
      "1529 of 6666 tweets downloaded in  73.46 s\n",
      "Updated date of last tweet: 2020-05-13 22:37:32+00:00\n",
      "sleeping for 2 minute...\n",
      "\n",
      "Scraping batch 8 ...\n",
      "6666 of 6666 tweets downloaded in  311.65 s\n",
      "Updated date of last tweet: 2020-04-19 23:00:20+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 9 ...\n",
      "6666 of 6666 tweets downloaded in  308.26 s\n",
      "Updated date of last tweet: 2020-03-25 03:03:02+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 10 ...\n",
      "6666 of 6666 tweets downloaded in  299.34 s\n",
      "Updated date of last tweet: 2020-03-03 17:00:20+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 11 ...\n",
      "6666 of 6666 tweets downloaded in  297.23 s\n",
      "Updated date of last tweet: 2020-02-13 00:22:40+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 12 ...\n",
      "6666 of 6666 tweets downloaded in  306.06 s\n",
      "Updated date of last tweet: 2020-01-24 07:12:45+00:00\n",
      "sleeping for 7 minutes...\n",
      "\n",
      "Scraping batch 13 ...\n",
      "6296 of 6666 tweets downloaded in  295.62 s\n",
      "Updated date of last tweet: 2020-01-01 00:00:00+00:00\n",
      "Limit date reached.\n"
     ]
    }
   ],
   "source": [
    "tweets, filename = gathering_data(name_of_coin, coin_symbol, nTweets, date_limit)\n",
    "tweets.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Getting_Data_Erwan.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
